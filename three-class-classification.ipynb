{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import outside libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data I/O\n",
    "import pandas as pd\n",
    "\n",
    "# neuron network\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# data standardization\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. read origin data\n",
    "def readData():\n",
    "    dt = pd.read_csv('demofile.csv',encoding='gbk')\n",
    "    # drop some useless column\n",
    "    dt.drop(['xxx1','xxx2','xxx3'],axis=1,inplace=True)\n",
    "    return dt\n",
    "dt = readData()\n",
    "# choose some useful column\n",
    "dt = dt[['xxx4','xxx5']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. construct training set & validation set:\n",
    "# construct labels\n",
    "labels = np.array(dt['xxx'])\n",
    "# construct features\n",
    "features = np.array(dt.drop('xxx', axis=1))\n",
    "# standardization\n",
    "inputFeatures = preprocessing.StandardScaler().fit_transform(features)\n",
    "# transform data type of labels and features from np to torch\n",
    "x = torch.tensor(inputFeatures, dtype=torch.float)\n",
    "y = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "dataScale = x.shape[0]   # size\n",
    "valScale = int(0.2 * dataScale)   # 20% data for validation set\n",
    "shuffledIndices = torch.randperm(dataScale)   # shuffle all of the data\n",
    "\n",
    "trainIndices = shuffledIndices[:-valScale]   # indices of train data\n",
    "validationIndices = shuffledIndices[-valScale:]   # indices of validation data\n",
    "\n",
    "trainLabels = y[trainIndices]\n",
    "trainFeatures = x[trainIndices]   # training set\n",
    "\n",
    "validateLabels = y[validationIndices]\n",
    "validateFeatures = x[validationIndices]   # validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ps: Because of the imbalance between different kinds of training data<br>\n",
    "(the num of normal is 5171, warning is 314 (1/17 times of normal), landslide is 552 (1/10 times of normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. balance different kinds of training data\n",
    "warningIndices = []\n",
    "landslideIndices  =[]\n",
    "\n",
    "for i in range(len(trainLabels)):\n",
    "    if trainLabels[i] == 1:\n",
    "        warningIndices.append(shuffledIndices[i])\n",
    "    elif trainLabels[i] == 2:\n",
    "        landslideIndices.append(shuffledIndices[i])\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "wl = y[warningIndices]\n",
    "wf = x[warningIndices]\n",
    "ll = y[landslideIndices]\n",
    "lf = x[landslideIndices]\n",
    "\n",
    "twl = torch.cat((wl, wl, wl, wl, wl, wl, wl, wl, wl, wl, wl, wl, wl, wl, wl, wl), 0)\n",
    "twf = torch.cat((wf, wf, wf, wf, wf, wf, wf, wf, wf, wf, wf, wf, wf, wf, wf, wf), 0)\n",
    "tll = torch.cat((ll, ll, ll, ll, ll, ll, ll, ll, ll), 0)\n",
    "tlf = torch.cat((lf, lf, lf, lf, lf, lf, lf, lf, lf), 0)\n",
    "\n",
    "trainLabels = torch.cat((trainLabels, twl, tll), 0)\n",
    "trainFeatures = torch.cat((trainFeatures, twf, tlf), 0)\n",
    "\n",
    "si = torch.randperm(trainLabels.shape[0])\n",
    "trainLabels = trainLabels[si]\n",
    "trainFeatures = trainFeatures[si]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. move to gpu\n",
    "x = x.cuda()\n",
    "y = y.cuda()\n",
    "trainLabels = trainLabels.cuda()\n",
    "trainFeatures = trainFeatures.cuda()\n",
    "validateLabels = validateLabels.cuda()\n",
    "validateFeatures = validateFeatures.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define neuron network and its paramsï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. define network for classification\n",
    "class ClassifyNetwork(nn.Module):\n",
    "    def __init__(self, featureNum):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num = featureNum\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.num, 50),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(50, 60),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(60, 50),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(50, 3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "    \n",
    "classifier = ClassifyNetwork(x.shape[1]).to(device='cuda')\n",
    "opt = optim.SGD(classifier.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. loss function\n",
    "def lossFunct(x, y):\n",
    "    CELoss = torch.nn.CrossEntropyLoss()\n",
    "    return CELoss(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. network training\n",
    "def training(epoches, optimizer, model, lossFunct, trainFeatures, trainLabels, validateFeatures, validateLabels):\n",
    "    for epoch in range(1, epoches+1):\n",
    "        trainPredictions = model(trainFeatures)\n",
    "        trainLoss = lossFunct(trainPredictions, trainLabels)\n",
    "        \n",
    "        validatePredictions = model(validateFeatures)\n",
    "        validateLoss = lossFunct(validatePredictions, validateLabels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        trainLoss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch == 1 or epoch % 500 == 0 or epoch == epoches:\n",
    "            print('Epoch {}, Training loss {}, Validation loss {}'.format(\n",
    "                epoch, float(trainLoss), float(validateLoss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training:\n",
    "(as tested before, epoches of 2500 is best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 1.1028711795806885, Validation loss 1.1323316097259521\n",
      "Epoch 500, Training loss 0.6354900598526001, Validation loss 0.6137858033180237\n",
      "Epoch 1000, Training loss 0.45828181505203247, Validation loss 0.4824090898036957\n",
      "Epoch 1500, Training loss 0.3360929787158966, Validation loss 0.35276153683662415\n",
      "Epoch 2000, Training loss 0.25133752822875977, Validation loss 0.29698774218559265\n",
      "Epoch 2500, Training loss 0.17777858674526215, Validation loss 0.22296260297298431\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training(\n",
    "    epoches = 2500, \n",
    "    optimizer = opt,\n",
    "    model = classifier,\n",
    "    lossFunct = lossFunct,\n",
    "    trainFeatures = trainFeatures,\n",
    "    trainLabels = trainLabels,\n",
    "    validateFeatures = validateFeatures,\n",
    "    validateLabels = validateLabels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get predicted class:\n",
    "def ans(x):\n",
    "    tmp = torch.max(x, 0)[1]\n",
    "    ans = 0\n",
    "    if tmp == 1:\n",
    "        ans = 1\n",
    "    elif tmp == 2:\n",
    "        ans = 2\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8902729923138086\n"
     ]
    }
   ],
   "source": [
    "# 2. get total accuracy:\n",
    "scale, correct = len(y), 0\n",
    "for i in range(scale):\n",
    "    if ans(classifier(x[i])) == y[i]:\n",
    "        correct += 1\n",
    "    \n",
    "print(correct / scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9549071618037135\n"
     ]
    }
   ],
   "source": [
    "# 3. get warning accuracy:\n",
    "scale, correct, size = 0, 0, len(y)\n",
    "for i in range(size):\n",
    "    if y[i] == 1:\n",
    "        scale += 1\n",
    "        if ans(classifier(x[i])) == 1:\n",
    "            correct += 1\n",
    "            \n",
    "print(correct / scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9653679653679653\n"
     ]
    }
   ],
   "source": [
    "# 4. get landslide accuracy:\n",
    "scale, correct, size = 0, 0, len(y)\n",
    "for i in range(size):\n",
    "    if y[i] == 2:\n",
    "        scale += 1\n",
    "        if ans(classifier(x[i])) == 2:\n",
    "            correct += 1\n",
    "            \n",
    "print(correct / scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.878474366893144\n"
     ]
    }
   ],
   "source": [
    "# 5. get normal accuracy:\n",
    "scale, correct, size = 0, 0, len(y)\n",
    "for i in range(size):\n",
    "    if y[i] == 0:\n",
    "        scale += 1\n",
    "        if ans(classifier(x[i])) == 0:\n",
    "            correct += 1\n",
    "            \n",
    "print(correct / scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print num of different class:\n",
    "(according to this answer, if we want to improve the accuracy of warning, we can increase the propertion of warning data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5168 5219 5620\n"
     ]
    }
   ],
   "source": [
    "scale = len(trainLabels)\n",
    "normal,warning,landslide = 0, 0, 0\n",
    "for i in range(scale):\n",
    "    if trainLabels[i] == 0:\n",
    "        normal += 1\n",
    "    elif trainLabels[i] == 1:\n",
    "        warning += 1\n",
    "    else:\n",
    "        landslide += 1\n",
    "        \n",
    "print(normal,warning,landslide)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
