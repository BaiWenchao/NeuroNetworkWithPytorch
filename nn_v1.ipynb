{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#参数\n",
    "num=100#畴数\n",
    "b_max=4.0e12#最大带宽\n",
    "l=3e-6#畴长"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数\n",
    "\n",
    "#折射率\n",
    "def n(omega):\n",
    "    if(omega>1e14):\n",
    "        t=100\n",
    "        a1=4.582\n",
    "        a2=0.09921\n",
    "        a3=0.2109\n",
    "        a4=-0.02194\n",
    "        b1=0.00000022971\n",
    "        b2=0.000000052716\n",
    "        b3=-0.000000049143\n",
    "        c=88506.25\n",
    "        t1=t+273.15\n",
    "        \n",
    "        w=(2*np.pi*3e8/omega)*1e6\n",
    "        c1=w*w-(a3+b3*(t1*t1-c))*(a3+b3*(t1*t1-c))\n",
    "        c2=a1+b1*(t1*t1-c)+(a2+b2*(t1*t1-c))/c1+a4*w*w\n",
    "        y=np.sqrt(c2)\n",
    "    else:\n",
    "        d1=19.9\n",
    "        d2=44\n",
    "        d3=4.533e12*2*np.pi\n",
    "        d4=0.426e12*2*np.pi\n",
    "        d5=2.9176e13\n",
    "        \n",
    "        e=d1+(d2-d1)*d5**2/(d3**2+1.0j*omega*d4-omega**2)\n",
    "        y=np.real(np.sqrt(e))\n",
    "\n",
    "    return y\n",
    "\n",
    "#波矢\n",
    "def kk(f):\n",
    "    z=f*n(f)/3e8\n",
    "    return z\n",
    "\n",
    "#带宽计算\n",
    "def funct(val):\n",
    "    G=np.zeros((2,500),dtype=np.complex)\n",
    "    d=np.zeros((2,num+1),dtype=np.complex)\n",
    "        \n",
    "    d[1,:-1] = val\n",
    "    d[1,num] = 0\n",
    "    \n",
    "        \n",
    "    for j in range(num+1):\n",
    "        d[0,j]=j*l\n",
    "        \n",
    "    for k in range(500):\n",
    "        f_thz=0.1e12+k*(b_max-0.1e12)/499\n",
    "        f_nir=3e8/800e-9\n",
    "        G[0,k]=-kk(2*np.pi*f_nir)+kk(2*np.pi*(f_nir-f_thz))+kk(2*np.pi*f_thz)\n",
    "        \n",
    "    for ii in np.arange(1,500):\n",
    "        for jj in np.arange(1,num):\n",
    "            G[1,ii]=G[1,ii]+d[1,jj]*(np.exp(1.0j*G[0,ii]*d[0,jj+1])-np.exp(1.0j*G[0,ii]*d[0,jj]))/(1.0j*G[0,ii])\n",
    "        \n",
    "    peak = np.max(np.abs(G[1,:]))\n",
    "    G_au = np.abs(G[1,:])/np.max(np.abs(G[1,:]))\n",
    "        \n",
    "    max_index = G_au.tolist().index(1)\n",
    "        \n",
    "    lo = max_index \n",
    "    # 前向搜索\n",
    "    while G_au[lo] > 0.5 and lo > 0:\n",
    "        lo -= 1\n",
    "        if G_au[lo] <= 0.5:\n",
    "            break \n",
    "\n",
    "    hi = max_index\n",
    "    # 后向搜索\n",
    "    while G_au[hi] > 0.5 and hi < 499:\n",
    "        hi += 1\n",
    "        if G_au[hi] <= 0.5:\n",
    "            break\n",
    "\n",
    "    if(hi==499 and G_au[hi]>0.5)or(lo==0 and G_au[lo]>0.5):\n",
    "        bw=-1\n",
    "    else:\n",
    "        bw = hi - lo\n",
    "    \n",
    "    bandwidth=bw*(b_max-0.1e12)/499e12\n",
    "    return bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义拟合网络：将输入的变量映射到最后的值：\n",
    "class FitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "                nn.Linear(num, 300),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(300, 500),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(500, 300),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(300, 1)\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "# 将定义的模型实例化：\n",
    "fitNet = FitModel()\n",
    "# 为实例化后的模型定义优化器\n",
    "fitOpt = optim.SGD(fitNet.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FitModel(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=300, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=300, out_features=500, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=500, out_features=300, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=300, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义生成网络：生成100个输入值：\n",
    "\n",
    "# 由于输入值只能为1或-1，所以选取tanh函数，通过对x放大，可以将实数域的值\n",
    "# 映射到近似1或-1两个值（可以绘制tanh(100x)的图形帮助理解）\n",
    "# 于是定义如下激活函数：\n",
    "class MyTanh(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyTanh, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(num*x)\n",
    "        return x \n",
    "\n",
    "# 接下来定义生成网络：\n",
    "class GenModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 网络1：得出输入        \n",
    "        self.net = nn.Sequential(\n",
    "                nn.Linear(num, 300),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(300, 500),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(500, 600),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(600, 500),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(500, 300),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(300, 100),\n",
    "                MyTanh()\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "genNet = GenModel()\n",
    "genOpt = optim.SGD(genNet.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出获取函数\n",
    "def getOutput(inList):\n",
    "    G=np.zeros([2,500],dtype=complex)\n",
    "    d=np.zeros([2,num+1],dtype=complex)\n",
    "\n",
    "    d[1,0:-1] = np.array(inList)\n",
    "    d[1,num]=0\n",
    "\n",
    "    for j in range(num+1):\n",
    "        d[0,j]=j*l\n",
    "\n",
    "    for k in range(500):\n",
    "        f_thz=0.1e12+k*(b_max-0.1e12)/499\n",
    "        f_nir=3e8/800e-9\n",
    "        G[0,k]=-kk(2*np.pi*f_nir)+kk(2*np.pi*(f_nir-f_thz))+kk(2*np.pi*f_thz)\n",
    "\n",
    "    for ii in np.arange(1,500):\n",
    "        for jj in np.arange(1,num):\n",
    "            G[1,ii]=G[1,ii]+d[1,jj]*(np.exp(1.0j*G[0,ii]*d[0,jj+1])-np.exp(1.0j*G[0,ii]*d[0,jj]))/(1.0j*G[0,ii])\n",
    "\n",
    "    peak = np.max(np.abs(G[1,:]))  \n",
    "    G_au = np.abs(G[1,:])/np.max(np.abs(G[1,:]))\n",
    "    max_index = G_au.tolist().index(1.0)\n",
    "\n",
    "    lo = max_index \n",
    "    # 前向搜索\n",
    "    while G_au[lo] >= 0.5 and lo > 0:\n",
    "        lo -= 1\n",
    "        if G_au[lo] < 0.5:\n",
    "            break \n",
    "\n",
    "    hi = max_index\n",
    "    # 后向搜索\n",
    "    while G_au[hi] >= 0.5 and hi < 499:\n",
    "        hi += 1\n",
    "        if G_au[hi] < 0.5:\n",
    "            break\n",
    "\n",
    "    if(hi==499 and G_au[hi]>0.5):\n",
    "        bw=500-lo\n",
    "    elif(lo==0 and G_au[lo]>0.5):\n",
    "        bw=hi-0\n",
    "    else:\n",
    "        bw = hi-lo        \n",
    "    \n",
    "    bandwidth=bw*(b_max-0.1e12)/499e12\n",
    "    return bandwidth\n",
    "\n",
    "# 输入输出获取函数\n",
    "def generateInput(size):\n",
    "    inputList = []\n",
    "    outputList=[]\n",
    "    i=0\n",
    "    while i<size: \n",
    "        inList = []\n",
    "        for j in range(num):\n",
    "            item = random.randint(0, 1)\n",
    "            if item == 0:\n",
    "                item = -1\n",
    "            inList.append(item)\n",
    "        outList=getOutput(inList)\n",
    "        i=i+1\n",
    "        inputList.append(inList)\n",
    "        outputList.append(outList)\n",
    "        \n",
    "    return inputList,outputList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取输入数据与输出数据：\n",
    "inputList,outputList = generateInput(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1407],\n",
       "        [0.0782],\n",
       "        [0.5002],\n",
       "        [0.1251],\n",
       "        [0.2579],\n",
       "        [0.2267],\n",
       "        [0.1798],\n",
       "        [0.0625],\n",
       "        [0.1954],\n",
       "        [0.3126],\n",
       "        [0.2579],\n",
       "        [0.1094],\n",
       "        [0.1798],\n",
       "        [0.1251],\n",
       "        [0.0547],\n",
       "        [0.1954],\n",
       "        [0.0625],\n",
       "        [0.1876],\n",
       "        [0.2579],\n",
       "        [0.2735],\n",
       "        [0.0547],\n",
       "        [0.1172],\n",
       "        [0.0782],\n",
       "        [0.1172],\n",
       "        [0.1798],\n",
       "        [0.1719],\n",
       "        [0.0860],\n",
       "        [0.1719],\n",
       "        [0.1329],\n",
       "        [0.2657],\n",
       "        [0.1798],\n",
       "        [0.0782],\n",
       "        [0.2345],\n",
       "        [0.3517],\n",
       "        [0.2657],\n",
       "        [0.1719],\n",
       "        [0.1563],\n",
       "        [0.1329],\n",
       "        [0.2188],\n",
       "        [0.1954],\n",
       "        [0.5549],\n",
       "        [0.2501],\n",
       "        [0.1329],\n",
       "        [0.2032],\n",
       "        [0.3439],\n",
       "        [0.1876],\n",
       "        [0.0938],\n",
       "        [0.1094],\n",
       "        [0.1563],\n",
       "        [0.1407],\n",
       "        [0.1563],\n",
       "        [0.2501],\n",
       "        [0.2423],\n",
       "        [0.0703],\n",
       "        [0.3126],\n",
       "        [0.0625],\n",
       "        [0.2267],\n",
       "        [0.2657],\n",
       "        [0.2032],\n",
       "        [0.2657],\n",
       "        [0.2032],\n",
       "        [0.1016],\n",
       "        [0.0703],\n",
       "        [0.1094],\n",
       "        [0.0547],\n",
       "        [0.3283],\n",
       "        [0.1094],\n",
       "        [0.1563],\n",
       "        [0.1563],\n",
       "        [0.0625],\n",
       "        [0.1251],\n",
       "        [0.1485],\n",
       "        [0.1954],\n",
       "        [0.2267],\n",
       "        [0.3752],\n",
       "        [0.1954],\n",
       "        [0.2970],\n",
       "        [0.0938],\n",
       "        [0.3830],\n",
       "        [0.2501],\n",
       "        [0.0625],\n",
       "        [0.4064],\n",
       "        [0.1407],\n",
       "        [0.0469],\n",
       "        [0.1485],\n",
       "        [0.0938],\n",
       "        [0.1329],\n",
       "        [0.1641],\n",
       "        [0.2110],\n",
       "        [0.2579],\n",
       "        [0.1641],\n",
       "        [0.2267],\n",
       "        [0.1719],\n",
       "        [0.2970],\n",
       "        [0.8285],\n",
       "        [0.2657],\n",
       "        [0.1016],\n",
       "        [0.2267],\n",
       "        [0.1016],\n",
       "        [0.2345],\n",
       "        [0.1719],\n",
       "        [0.2814],\n",
       "        [0.2579],\n",
       "        [0.0625],\n",
       "        [0.1094],\n",
       "        [0.1563],\n",
       "        [0.3752],\n",
       "        [0.1563],\n",
       "        [0.1563],\n",
       "        [0.3673],\n",
       "        [0.2032],\n",
       "        [0.1641],\n",
       "        [0.1563],\n",
       "        [0.1172],\n",
       "        [0.2032],\n",
       "        [0.0234],\n",
       "        [0.2032],\n",
       "        [0.2345],\n",
       "        [0.1641],\n",
       "        [0.0469],\n",
       "        [0.2345],\n",
       "        [0.2501],\n",
       "        [0.2735],\n",
       "        [0.2188],\n",
       "        [0.0703],\n",
       "        [0.1407],\n",
       "        [0.2110],\n",
       "        [0.3361],\n",
       "        [0.2423],\n",
       "        [0.1329],\n",
       "        [0.2579],\n",
       "        [0.1172],\n",
       "        [0.0782],\n",
       "        [0.0938],\n",
       "        [0.1563],\n",
       "        [0.1172],\n",
       "        [0.2032],\n",
       "        [0.1563],\n",
       "        [0.2188],\n",
       "        [0.1016],\n",
       "        [0.0782],\n",
       "        [0.1641],\n",
       "        [0.3048],\n",
       "        [0.1719],\n",
       "        [0.3126],\n",
       "        [0.1016],\n",
       "        [0.1329],\n",
       "        [0.2814],\n",
       "        [0.1329],\n",
       "        [0.2032],\n",
       "        [0.1016],\n",
       "        [0.2345],\n",
       "        [0.1641],\n",
       "        [0.2032],\n",
       "        [0.3439],\n",
       "        [0.0782],\n",
       "        [0.1172],\n",
       "        [0.1876],\n",
       "        [0.1407],\n",
       "        [0.2579],\n",
       "        [0.2110],\n",
       "        [0.2501],\n",
       "        [0.2188],\n",
       "        [0.1954],\n",
       "        [0.0860],\n",
       "        [0.2735],\n",
       "        [0.1798],\n",
       "        [0.2267],\n",
       "        [0.2188],\n",
       "        [0.1719],\n",
       "        [0.0547],\n",
       "        [0.0547],\n",
       "        [0.3908],\n",
       "        [0.1016],\n",
       "        [0.2579],\n",
       "        [0.1641],\n",
       "        [0.1251],\n",
       "        [0.0938],\n",
       "        [0.1954],\n",
       "        [0.1876],\n",
       "        [0.1798],\n",
       "        [0.1954],\n",
       "        [0.1641],\n",
       "        [0.2970],\n",
       "        [0.1251],\n",
       "        [0.1563],\n",
       "        [0.4533],\n",
       "        [0.2032],\n",
       "        [0.4142],\n",
       "        [0.2345],\n",
       "        [0.4377],\n",
       "        [0.1876],\n",
       "        [0.2345],\n",
       "        [0.1094],\n",
       "        [0.3986],\n",
       "        [0.4142],\n",
       "        [0.0703],\n",
       "        [0.4220],\n",
       "        [0.1407],\n",
       "        [0.2267],\n",
       "        [0.0938],\n",
       "        [0.1407],\n",
       "        [0.0782],\n",
       "        [0.1798],\n",
       "        [0.3048],\n",
       "        [0.1876],\n",
       "        [0.2423],\n",
       "        [0.2814],\n",
       "        [0.3048],\n",
       "        [0.3283],\n",
       "        [0.0625],\n",
       "        [0.1407],\n",
       "        [0.1485],\n",
       "        [0.1719],\n",
       "        [0.1563],\n",
       "        [0.2814],\n",
       "        [0.1172],\n",
       "        [0.1485],\n",
       "        [0.1251],\n",
       "        [0.2267],\n",
       "        [0.2735],\n",
       "        [0.0625],\n",
       "        [0.2892],\n",
       "        [0.0547],\n",
       "        [0.1876],\n",
       "        [0.1016],\n",
       "        [0.0782],\n",
       "        [0.3204],\n",
       "        [0.2032],\n",
       "        [0.0860],\n",
       "        [0.2814],\n",
       "        [0.2345],\n",
       "        [0.2814],\n",
       "        [0.1641],\n",
       "        [0.0938],\n",
       "        [0.2345],\n",
       "        [0.2345],\n",
       "        [0.2032],\n",
       "        [0.1329],\n",
       "        [0.1876],\n",
       "        [0.1798],\n",
       "        [0.1485],\n",
       "        [0.3283],\n",
       "        [0.0782],\n",
       "        [0.0703],\n",
       "        [0.1485],\n",
       "        [0.3361],\n",
       "        [0.1641],\n",
       "        [0.0625],\n",
       "        [0.2501],\n",
       "        [0.1485],\n",
       "        [0.1719],\n",
       "        [0.1016],\n",
       "        [0.2657],\n",
       "        [0.1016],\n",
       "        [0.0938],\n",
       "        [0.0782],\n",
       "        [0.1954],\n",
       "        [0.1172],\n",
       "        [0.2032],\n",
       "        [0.6253],\n",
       "        [0.0313],\n",
       "        [0.1094],\n",
       "        [0.2501],\n",
       "        [0.1485],\n",
       "        [0.0703],\n",
       "        [0.3908],\n",
       "        [0.0469],\n",
       "        [0.1954],\n",
       "        [0.1329],\n",
       "        [0.0625],\n",
       "        [0.2892],\n",
       "        [0.2423],\n",
       "        [0.1407],\n",
       "        [0.0703],\n",
       "        [0.2892],\n",
       "        [0.1876],\n",
       "        [0.1251],\n",
       "        [0.2188],\n",
       "        [0.1876],\n",
       "        [0.3439],\n",
       "        [0.1876],\n",
       "        [0.1954],\n",
       "        [0.0782],\n",
       "        [0.3204],\n",
       "        [0.2345],\n",
       "        [0.1954],\n",
       "        [0.1251],\n",
       "        [0.2188],\n",
       "        [0.2657],\n",
       "        [0.2970],\n",
       "        [0.1876],\n",
       "        [0.1094],\n",
       "        [0.2501],\n",
       "        [0.0782],\n",
       "        [0.0625],\n",
       "        [0.3830],\n",
       "        [0.1954],\n",
       "        [0.2735],\n",
       "        [0.0625],\n",
       "        [0.1485],\n",
       "        [0.2814],\n",
       "        [0.0782],\n",
       "        [0.0391],\n",
       "        [0.0938],\n",
       "        [0.1954],\n",
       "        [0.2032],\n",
       "        [0.2501],\n",
       "        [0.0391],\n",
       "        [0.2345],\n",
       "        [0.1485],\n",
       "        [0.0234],\n",
       "        [0.2579],\n",
       "        [0.0860],\n",
       "        [0.3908],\n",
       "        [0.3908],\n",
       "        [0.0938],\n",
       "        [0.1407],\n",
       "        [0.0547],\n",
       "        [0.1094],\n",
       "        [0.2345],\n",
       "        [0.3126],\n",
       "        [0.1172],\n",
       "        [0.2735],\n",
       "        [0.0547],\n",
       "        [0.3517],\n",
       "        [0.1172],\n",
       "        [0.1719],\n",
       "        [0.1407],\n",
       "        [0.1798],\n",
       "        [0.0703],\n",
       "        [0.1719],\n",
       "        [0.1563],\n",
       "        [0.2267],\n",
       "        [0.1798],\n",
       "        [0.2110],\n",
       "        [0.2267],\n",
       "        [0.2735],\n",
       "        [0.2579],\n",
       "        [0.1563],\n",
       "        [0.1407],\n",
       "        [0.4924],\n",
       "        [0.1329],\n",
       "        [0.1798],\n",
       "        [0.3204],\n",
       "        [0.3361],\n",
       "        [0.2501],\n",
       "        [0.2345],\n",
       "        [0.1485],\n",
       "        [0.0782],\n",
       "        [0.1329],\n",
       "        [0.1251],\n",
       "        [0.2735],\n",
       "        [0.0391],\n",
       "        [0.2501],\n",
       "        [0.1876],\n",
       "        [0.0391],\n",
       "        [0.1094],\n",
       "        [0.0938],\n",
       "        [0.2970],\n",
       "        [0.2110],\n",
       "        [0.2814],\n",
       "        [0.1719],\n",
       "        [0.1719],\n",
       "        [0.3204],\n",
       "        [0.1719],\n",
       "        [0.2110],\n",
       "        [0.0625],\n",
       "        [0.1641],\n",
       "        [0.3283],\n",
       "        [0.1876],\n",
       "        [0.1094],\n",
       "        [0.0782],\n",
       "        [0.2579],\n",
       "        [0.1563],\n",
       "        [0.0782],\n",
       "        [0.1954],\n",
       "        [0.1798],\n",
       "        [0.0703],\n",
       "        [0.2345],\n",
       "        [0.1876],\n",
       "        [0.2345],\n",
       "        [0.1251],\n",
       "        [0.1329],\n",
       "        [0.1172],\n",
       "        [0.2345],\n",
       "        [0.2657],\n",
       "        [0.0938],\n",
       "        [0.4064],\n",
       "        [0.0391],\n",
       "        [0.0469],\n",
       "        [0.1172],\n",
       "        [0.1016],\n",
       "        [0.1172],\n",
       "        [0.1954],\n",
       "        [1.0395],\n",
       "        [0.5393],\n",
       "        [0.1876],\n",
       "        [0.2188],\n",
       "        [0.1251],\n",
       "        [0.2423],\n",
       "        [0.0469],\n",
       "        [0.1876],\n",
       "        [0.3752],\n",
       "        [0.1485],\n",
       "        [0.1719],\n",
       "        [0.1954],\n",
       "        [0.2345],\n",
       "        [0.1094],\n",
       "        [0.1876],\n",
       "        [0.2345],\n",
       "        [0.6018],\n",
       "        [0.0782],\n",
       "        [0.0860],\n",
       "        [0.2892],\n",
       "        [0.1563],\n",
       "        [0.1563],\n",
       "        [0.3517],\n",
       "        [0.2267],\n",
       "        [0.1172],\n",
       "        [0.1094],\n",
       "        [0.3283],\n",
       "        [0.2345],\n",
       "        [0.0391],\n",
       "        [0.0860],\n",
       "        [0.2032],\n",
       "        [0.3673],\n",
       "        [0.5080],\n",
       "        [0.0860],\n",
       "        [0.1329],\n",
       "        [0.3673],\n",
       "        [0.0938],\n",
       "        [0.1329],\n",
       "        [0.1329],\n",
       "        [0.2032],\n",
       "        [0.1094],\n",
       "        [0.0860],\n",
       "        [0.2345],\n",
       "        [0.0938],\n",
       "        [0.0938],\n",
       "        [0.2423],\n",
       "        [0.0547],\n",
       "        [0.2501],\n",
       "        [1.3130],\n",
       "        [0.1954],\n",
       "        [0.3439],\n",
       "        [0.2188],\n",
       "        [0.1954],\n",
       "        [0.1094],\n",
       "        [0.0703],\n",
       "        [0.1719],\n",
       "        [0.5705],\n",
       "        [0.2501],\n",
       "        [0.1094],\n",
       "        [0.0703],\n",
       "        [0.0938],\n",
       "        [0.2423],\n",
       "        [0.1641],\n",
       "        [0.3283],\n",
       "        [0.2814],\n",
       "        [0.1172],\n",
       "        [0.2423],\n",
       "        [0.0938],\n",
       "        [0.2267],\n",
       "        [0.2657],\n",
       "        [0.3048],\n",
       "        [0.1407],\n",
       "        [0.1407],\n",
       "        [0.1876],\n",
       "        [0.3361],\n",
       "        [0.0860],\n",
       "        [0.1251],\n",
       "        [0.3830],\n",
       "        [0.3595],\n",
       "        [0.4299],\n",
       "        [0.1719],\n",
       "        [0.1094],\n",
       "        [0.1641],\n",
       "        [0.3361],\n",
       "        [0.1172],\n",
       "        [0.0469],\n",
       "        [0.1407],\n",
       "        [0.1172],\n",
       "        [0.7659],\n",
       "        [0.3908],\n",
       "        [0.1172],\n",
       "        [0.2892],\n",
       "        [0.2892],\n",
       "        [0.1563],\n",
       "        [0.0782],\n",
       "        [0.2423],\n",
       "        [0.3204],\n",
       "        [0.4611],\n",
       "        [0.2501],\n",
       "        [0.2970],\n",
       "        [0.1485],\n",
       "        [0.0860],\n",
       "        [0.2110],\n",
       "        [0.1016],\n",
       "        [0.0469],\n",
       "        [0.0469],\n",
       "        [0.2657],\n",
       "        [0.0703],\n",
       "        [0.0860],\n",
       "        [0.1172],\n",
       "        [0.2267],\n",
       "        [0.1016],\n",
       "        [0.1876],\n",
       "        [0.1876],\n",
       "        [0.1251],\n",
       "        [0.2423],\n",
       "        [0.2110],\n",
       "        [0.1876],\n",
       "        [0.0938],\n",
       "        [0.1876],\n",
       "        [0.1251],\n",
       "        [0.2735],\n",
       "        [0.2423],\n",
       "        [0.2501],\n",
       "        [0.0860],\n",
       "        [0.2970],\n",
       "        [0.1407],\n",
       "        [0.0469],\n",
       "        [0.3204],\n",
       "        [0.6643],\n",
       "        [0.2814],\n",
       "        [0.2267],\n",
       "        [0.2188],\n",
       "        [0.2423],\n",
       "        [0.1251],\n",
       "        [0.2345],\n",
       "        [0.0938],\n",
       "        [0.2110],\n",
       "        [0.0547],\n",
       "        [0.2110],\n",
       "        [0.2110],\n",
       "        [0.2892],\n",
       "        [0.2423],\n",
       "        [0.1563],\n",
       "        [0.2188],\n",
       "        [0.2032],\n",
       "        [0.2267],\n",
       "        [0.1016],\n",
       "        [0.2892],\n",
       "        [0.3048],\n",
       "        [0.3204],\n",
       "        [0.3517],\n",
       "        [0.2579],\n",
       "        [0.1719],\n",
       "        [0.2345],\n",
       "        [0.2735],\n",
       "        [0.5002],\n",
       "        [0.2657],\n",
       "        [0.3439],\n",
       "        [0.0703],\n",
       "        [0.1876],\n",
       "        [0.0703],\n",
       "        [0.2657],\n",
       "        [0.2735],\n",
       "        [0.1719],\n",
       "        [0.1407],\n",
       "        [0.4533],\n",
       "        [0.1954],\n",
       "        [0.5393],\n",
       "        [0.1094],\n",
       "        [0.4533],\n",
       "        [0.1251],\n",
       "        [0.2110],\n",
       "        [0.1641],\n",
       "        [0.0938],\n",
       "        [0.0860],\n",
       "        [0.0860],\n",
       "        [0.1641],\n",
       "        [0.2032],\n",
       "        [0.2110],\n",
       "        [0.2032],\n",
       "        [0.1251],\n",
       "        [0.2188],\n",
       "        [0.1251],\n",
       "        [0.1798],\n",
       "        [0.3908],\n",
       "        [0.0782],\n",
       "        [0.1172],\n",
       "        [0.2188],\n",
       "        [0.1016],\n",
       "        [0.1016],\n",
       "        [0.2579],\n",
       "        [0.2970],\n",
       "        [0.1094],\n",
       "        [0.2345],\n",
       "        [0.2579],\n",
       "        [0.4064],\n",
       "        [0.3204],\n",
       "        [0.2735],\n",
       "        [0.2970],\n",
       "        [0.1251],\n",
       "        [0.1016],\n",
       "        [0.2423],\n",
       "        [0.0703],\n",
       "        [0.3361],\n",
       "        [0.2267],\n",
       "        [0.5549],\n",
       "        [0.2501],\n",
       "        [0.0469],\n",
       "        [0.2735],\n",
       "        [0.1485],\n",
       "        [0.0938],\n",
       "        [0.2657],\n",
       "        [0.2657],\n",
       "        [0.2814],\n",
       "        [0.2423],\n",
       "        [0.2188],\n",
       "        [0.3204],\n",
       "        [0.0938],\n",
       "        [0.0782],\n",
       "        [0.1407],\n",
       "        [0.2110],\n",
       "        [0.1798],\n",
       "        [0.1563],\n",
       "        [0.2970],\n",
       "        [0.1094],\n",
       "        [0.3517],\n",
       "        [0.5471],\n",
       "        [0.1329],\n",
       "        [0.1094],\n",
       "        [0.2892],\n",
       "        [0.2501],\n",
       "        [0.1251],\n",
       "        [0.1329],\n",
       "        [0.0547],\n",
       "        [0.0625],\n",
       "        [0.2345],\n",
       "        [0.0782],\n",
       "        [0.2579],\n",
       "        [0.1563],\n",
       "        [0.3361],\n",
       "        [0.1016],\n",
       "        [0.1016],\n",
       "        [0.1251],\n",
       "        [0.1094],\n",
       "        [0.1563],\n",
       "        [0.1485],\n",
       "        [0.1954],\n",
       "        [0.2501],\n",
       "        [0.3439],\n",
       "        [0.1641],\n",
       "        [0.1563],\n",
       "        [0.1016],\n",
       "        [0.1251],\n",
       "        [0.2267],\n",
       "        [0.2267],\n",
       "        [0.2267],\n",
       "        [0.2345],\n",
       "        [0.0625],\n",
       "        [0.0782],\n",
       "        [0.0938],\n",
       "        [0.2814],\n",
       "        [0.2735],\n",
       "        [0.3361],\n",
       "        [0.2579],\n",
       "        [0.1719],\n",
       "        [0.2110],\n",
       "        [0.1719],\n",
       "        [0.3361],\n",
       "        [0.2735],\n",
       "        [0.0938],\n",
       "        [0.2423],\n",
       "        [0.2110],\n",
       "        [0.1563],\n",
       "        [0.2501],\n",
       "        [0.1641],\n",
       "        [0.8128],\n",
       "        [0.0782],\n",
       "        [0.1719],\n",
       "        [0.1485],\n",
       "        [0.2423],\n",
       "        [0.1954],\n",
       "        [0.1016],\n",
       "        [0.0391],\n",
       "        [0.1954],\n",
       "        [0.2892],\n",
       "        [0.1407],\n",
       "        [0.4299],\n",
       "        [0.0625],\n",
       "        [0.3986],\n",
       "        [0.2735],\n",
       "        [0.1719],\n",
       "        [0.0938],\n",
       "        [0.1407],\n",
       "        [0.0547],\n",
       "        [0.2032],\n",
       "        [0.1094],\n",
       "        [0.1329],\n",
       "        [0.0703],\n",
       "        [0.0625],\n",
       "        [0.0782],\n",
       "        [0.2970],\n",
       "        [0.0313],\n",
       "        [0.1094],\n",
       "        [0.1798],\n",
       "        [0.1251],\n",
       "        [0.1485],\n",
       "        [0.2267],\n",
       "        [0.1407],\n",
       "        [0.1485],\n",
       "        [0.2345],\n",
       "        [0.0938],\n",
       "        [0.2501],\n",
       "        [0.0625],\n",
       "        [0.1094],\n",
       "        [0.1641],\n",
       "        [0.1407],\n",
       "        [0.1641],\n",
       "        [0.1172],\n",
       "        [0.2735],\n",
       "        [0.1798],\n",
       "        [0.1329],\n",
       "        [0.0782],\n",
       "        [0.0860],\n",
       "        [0.0860],\n",
       "        [0.0547],\n",
       "        [0.2110],\n",
       "        [0.6096],\n",
       "        [0.2032],\n",
       "        [0.1798],\n",
       "        [0.3517],\n",
       "        [0.4846],\n",
       "        [0.3517],\n",
       "        [0.2501],\n",
       "        [0.2657],\n",
       "        [0.2032],\n",
       "        [0.2814],\n",
       "        [0.3595],\n",
       "        [0.3283],\n",
       "        [0.2032],\n",
       "        [0.1016],\n",
       "        [0.1876],\n",
       "        [0.2579],\n",
       "        [0.2267],\n",
       "        [0.6487],\n",
       "        [0.0938],\n",
       "        [0.1641],\n",
       "        [0.1954],\n",
       "        [0.3361],\n",
       "        [0.2579],\n",
       "        [0.3908],\n",
       "        [0.3361],\n",
       "        [0.1172],\n",
       "        [0.1172],\n",
       "        [0.6018],\n",
       "        [0.3439],\n",
       "        [0.1563],\n",
       "        [0.3439],\n",
       "        [0.0782],\n",
       "        [0.2501],\n",
       "        [0.1251],\n",
       "        [0.3361],\n",
       "        [0.1641],\n",
       "        [0.2110],\n",
       "        [0.1798],\n",
       "        [0.0703],\n",
       "        [0.2110],\n",
       "        [0.2892],\n",
       "        [0.0703],\n",
       "        [0.2970],\n",
       "        [0.0860],\n",
       "        [0.0547],\n",
       "        [0.2267],\n",
       "        [0.0938],\n",
       "        [0.1876],\n",
       "        [0.0469],\n",
       "        [0.1172],\n",
       "        [0.3283],\n",
       "        [0.0469],\n",
       "        [0.0391],\n",
       "        [0.1563],\n",
       "        [0.1719],\n",
       "        [0.5158],\n",
       "        [0.1172],\n",
       "        [0.2814],\n",
       "        [0.2267],\n",
       "        [0.2188],\n",
       "        [0.0625],\n",
       "        [0.1954],\n",
       "        [0.4455],\n",
       "        [0.2423],\n",
       "        [0.1329],\n",
       "        [0.2501],\n",
       "        [0.2579],\n",
       "        [0.2814],\n",
       "        [0.0625],\n",
       "        [0.1876],\n",
       "        [0.1172],\n",
       "        [0.2267],\n",
       "        [0.1798],\n",
       "        [0.2501],\n",
       "        [0.3752],\n",
       "        [0.2735],\n",
       "        [0.2188],\n",
       "        [0.2501],\n",
       "        [0.2267],\n",
       "        [0.3830],\n",
       "        [0.1485],\n",
       "        [0.0703],\n",
       "        [0.3283],\n",
       "        [0.2267],\n",
       "        [0.1563],\n",
       "        [0.1251],\n",
       "        [0.4768],\n",
       "        [0.2579],\n",
       "        [0.0703],\n",
       "        [0.4377],\n",
       "        [0.2188],\n",
       "        [0.0703],\n",
       "        [0.1876],\n",
       "        [0.1563],\n",
       "        [0.2501],\n",
       "        [0.3752],\n",
       "        [0.1954],\n",
       "        [0.1172],\n",
       "        [0.1329],\n",
       "        [0.3361],\n",
       "        [0.0938],\n",
       "        [0.1016],\n",
       "        [0.2501],\n",
       "        [0.1719],\n",
       "        [0.1172],\n",
       "        [0.2579],\n",
       "        [0.2345],\n",
       "        [0.1016],\n",
       "        [0.2032],\n",
       "        [0.1719],\n",
       "        [0.2501],\n",
       "        [0.0625],\n",
       "        [0.0782],\n",
       "        [0.1719],\n",
       "        [0.2423],\n",
       "        [0.1954],\n",
       "        [0.1016],\n",
       "        [0.5393],\n",
       "        [0.5002],\n",
       "        [0.1094],\n",
       "        [0.1563],\n",
       "        [0.1485],\n",
       "        [0.4924],\n",
       "        [0.2657],\n",
       "        [0.1172],\n",
       "        [0.1407],\n",
       "        [0.1251],\n",
       "        [0.2110],\n",
       "        [0.2267],\n",
       "        [0.2814],\n",
       "        [0.2501],\n",
       "        [0.1485],\n",
       "        [0.1485],\n",
       "        [0.2188],\n",
       "        [0.3595],\n",
       "        [0.1016],\n",
       "        [0.3986],\n",
       "        [0.1954],\n",
       "        [0.1251],\n",
       "        [0.2814],\n",
       "        [0.3126],\n",
       "        [0.3126],\n",
       "        [0.0782],\n",
       "        [0.2032],\n",
       "        [0.0782],\n",
       "        [0.0860],\n",
       "        [0.0860],\n",
       "        [0.1719],\n",
       "        [0.3517],\n",
       "        [0.1798],\n",
       "        [0.1251],\n",
       "        [0.1094],\n",
       "        [0.4064],\n",
       "        [0.1407],\n",
       "        [0.1094],\n",
       "        [0.0234],\n",
       "        [0.2579],\n",
       "        [0.0625],\n",
       "        [0.0860],\n",
       "        [0.1876],\n",
       "        [0.2032],\n",
       "        [0.2188],\n",
       "        [0.0547],\n",
       "        [0.1251],\n",
       "        [0.1251],\n",
       "        [0.1563],\n",
       "        [0.2345],\n",
       "        [0.1329],\n",
       "        [0.3517],\n",
       "        [0.2501],\n",
       "        [0.1094],\n",
       "        [0.1251],\n",
       "        [0.1016],\n",
       "        [0.1329],\n",
       "        [0.2657],\n",
       "        [0.4299],\n",
       "        [0.2345],\n",
       "        [0.6331],\n",
       "        [0.1016],\n",
       "        [0.1876],\n",
       "        [0.1251],\n",
       "        [0.2657],\n",
       "        [0.2188],\n",
       "        [0.2501],\n",
       "        [0.2188],\n",
       "        [0.1641],\n",
       "        [0.2032],\n",
       "        [0.2032],\n",
       "        [0.0547],\n",
       "        [0.1798],\n",
       "        [0.1407],\n",
       "        [0.2970],\n",
       "        [0.2345],\n",
       "        [0.2110],\n",
       "        [0.3048],\n",
       "        [0.1329],\n",
       "        [0.0860],\n",
       "        [0.2579],\n",
       "        [0.1016],\n",
       "        [0.1172],\n",
       "        [0.1016],\n",
       "        [0.0547],\n",
       "        [0.0782],\n",
       "        [0.2970],\n",
       "        [0.2032],\n",
       "        [0.1016],\n",
       "        [0.1407],\n",
       "        [0.3752],\n",
       "        [0.2501],\n",
       "        [0.0625],\n",
       "        [0.2892],\n",
       "        [0.0703],\n",
       "        [0.2735],\n",
       "        [0.0703],\n",
       "        [0.2032],\n",
       "        [0.1719],\n",
       "        [0.2267],\n",
       "        [0.2892],\n",
       "        [0.0703],\n",
       "        [0.2110],\n",
       "        [0.1251],\n",
       "        [0.3908],\n",
       "        [0.1954],\n",
       "        [0.3986],\n",
       "        [0.1719],\n",
       "        [0.2267],\n",
       "        [0.2345],\n",
       "        [0.0703],\n",
       "        [0.1251],\n",
       "        [0.3986],\n",
       "        [0.0547],\n",
       "        [0.1954],\n",
       "        [0.2579],\n",
       "        [0.3048],\n",
       "        [0.1251],\n",
       "        [0.1719],\n",
       "        [0.1641],\n",
       "        [0.2345],\n",
       "        [0.0938],\n",
       "        [0.0860],\n",
       "        [0.1485],\n",
       "        [0.2657],\n",
       "        [0.1094],\n",
       "        [0.4533],\n",
       "        [0.0547],\n",
       "        [0.0625],\n",
       "        [0.3126],\n",
       "        [0.4142],\n",
       "        [0.2188],\n",
       "        [0.3204],\n",
       "        [0.0860],\n",
       "        [0.4846],\n",
       "        [0.1407],\n",
       "        [0.2423],\n",
       "        [0.0625],\n",
       "        [0.1094],\n",
       "        [0.2032],\n",
       "        [0.1016],\n",
       "        [0.3517],\n",
       "        [0.2345],\n",
       "        [0.3908],\n",
       "        [0.3048],\n",
       "        [0.1329],\n",
       "        [0.3048],\n",
       "        [0.2735],\n",
       "        [0.1016],\n",
       "        [0.0625],\n",
       "        [0.4611],\n",
       "        [0.0938],\n",
       "        [0.2267],\n",
       "        [0.1563],\n",
       "        [0.2501],\n",
       "        [0.0938],\n",
       "        [0.1251],\n",
       "        [0.1172],\n",
       "        [0.2423],\n",
       "        [0.5705],\n",
       "        [0.0547]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0937875751503006"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inList=[-1., -1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,\n",
    "         1.,  1., -1.,  1., -1., -1., -1., -1.,  1., -1.,  1., -1.,  1., -1.,\n",
    "        -1., -1., -1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1.,  1.,  1.,\n",
    "        -1., -1., -1.,  1., -1., -1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,\n",
    "         1.,  1., -1., -1., -1.,  1., -1., -1., -1., -1.,  1.,  1.,  1., -1.,\n",
    "         1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1., -1., -1.,  1., -1.,\n",
    "         1., -1., -1.,  1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,\n",
    "        -1., -1.]\n",
    "\n",
    "getOutput(inList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20241\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "C:\\Users\\20241\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# 对输入数据与输出数据tensor包装，以便输入神经网络进行训练：\n",
    "inputList = torch.tensor(inputList)\n",
    "outputList = torch.tensor(outputList).unsqueeze(1)\n",
    "\n",
    "# 进行训练集与测试集划分：\n",
    "n_samples = inputList.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "input_train = inputList[train_indices]     # 训练集：输入数据\n",
    "output_train = outputList[train_indices]   # 训练集：输出数据\n",
    "\n",
    "input_val = inputList[val_indices]         # 测试集：输入数据\n",
    "output_val = outputList[val_indices]       # 测试集：输出数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义拟合网络的训练函数：\n",
    "def fitTrain(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val, t_c_train, t_c_val):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_p_train = model(t_u_train)\n",
    "        loss_train = loss_fn(t_p_train, t_c_train)\n",
    "\n",
    "        t_p_val = model(t_u_val)\n",
    "        loss_val = loss_fn(t_p_val, t_c_val)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch == 1 or epoch % 50 == 0:\n",
    "            print('Epoch {}, Training loss {}, Validation loss {}'.format(\n",
    "                epoch, float(loss_train), float(loss_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FitModel(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=300, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=300, out_features=500, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=500, out_features=300, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=300, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20241\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:431: UserWarning: Using a target size (torch.Size([800, 1, 1])) that is different to the input size (torch.Size([800, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\20241\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:431: UserWarning: Using a target size (torch.Size([200, 1, 1])) that is different to the input size (torch.Size([200, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 0.014616081491112709, Validation loss 0.018139345571398735\n",
      "Epoch 50, Training loss 0.01458809245377779, Validation loss 0.018125319853425026\n",
      "Epoch 100, Training loss 0.01456172950565815, Validation loss 0.018105003982782364\n",
      "Epoch 150, Training loss 0.014537307433784008, Validation loss 0.01808582805097103\n",
      "Epoch 200, Training loss 0.014514596201479435, Validation loss 0.018067918717861176\n"
     ]
    }
   ],
   "source": [
    "# 进行拟合函数的训练：\n",
    "fitTrain(\n",
    "    n_epochs = 200, \n",
    "    optimizer = fitOpt,\n",
    "    model = fitNet,\n",
    "    loss_fn = nn.MSELoss(),\n",
    "    t_u_train = input_train.float(),\n",
    "    t_u_val = input_val.float(), \n",
    "    t_c_train = output_train.float(),\n",
    "    t_c_val = output_val.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拟合网络训练结束后，冻结其全部参数，以免在生成网络的训练过程中被修改：\n",
    "fitNet.require_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义生成网络的训练：\n",
    "def genTrain(n_epochs, optimizer, model):\n",
    "    val = torch.ones(num)\n",
    "    out = 0\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        out = model(val)\n",
    "        \n",
    "        # 我们希望fitNet(out)的值越大越好\n",
    "#         loss = torch.exp(1/fitNet(out))\n",
    "        loss = -fitNet(out)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch == 1 or epoch % 1000 == 0:\n",
    "            print('Epoch {}, loss {}, out {}'.format(\n",
    "                epoch, float(loss), out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss -0.15582701563835144, out tensor([-0.7077, -0.9999,  0.6496, -1.0000, -0.8532, -0.9996,  0.9142, -1.0000,\n",
      "         0.9988,  0.9985, -1.0000,  0.9996,  0.9970,  0.9989, -1.0000,  0.9887,\n",
      "         1.0000, -0.9393, -0.7495,  0.9829,  0.9999,  0.9833, -0.9961, -0.8786,\n",
      "        -0.6983,  1.0000,  1.0000,  0.9909, -0.5559,  0.2996,  0.8084,  0.9919,\n",
      "         0.9836,  0.9964,  0.9998,  1.0000, -0.9961,  1.0000,  0.9937,  0.7204,\n",
      "         0.8879, -0.9994, -0.7333,  0.9931, -0.3534, -0.8850,  0.9999,  0.9188,\n",
      "         1.0000,  0.8583,  0.9757, -0.7806, -0.4498, -0.9998,  0.9247, -0.9997,\n",
      "         0.7112, -0.0168, -1.0000, -0.9871, -0.9999, -0.9729,  0.9973, -0.8948,\n",
      "         0.9705, -0.9934, -0.3702,  1.0000,  0.5612,  0.9717, -0.8672, -0.9996,\n",
      "        -0.8964,  0.9028, -0.9969, -1.0000,  0.9960,  0.5293,  0.5034,  0.9950,\n",
      "        -0.9997,  1.0000,  0.9999,  0.8454, -1.0000,  0.9988, -0.9998,  0.8877,\n",
      "         0.7981,  0.9876,  0.9995,  0.6453, -0.4218,  1.0000, -0.8261,  0.9989,\n",
      "         0.1387, -0.7132,  0.9678, -0.9999], grad_fn=<TanhBackward>)\n",
      "Epoch 1000, loss -0.4247509837150574, out tensor([-1.0000, -0.9794,  1.0000, -1.0000, -0.9999, -0.9999, -0.9999, -1.0000,\n",
      "         0.9999, -0.9998, -1.0000, -0.9999,  1.0000,  1.0000, -1.0000, -0.9997,\n",
      "         1.0000, -1.0000,  0.9999,  0.9999, -0.7063, -0.9997, -0.9999, -0.9999,\n",
      "        -0.9997, -0.9998,  1.0000, -0.9999,  1.0000,  0.9997, -0.9998, -0.9971,\n",
      "        -0.9998,  1.0000,  1.0000,  1.0000,  0.9999,  1.0000,  0.9999,  1.0000,\n",
      "         0.9999, -0.9964,  1.0000,  0.9996, -0.9999,  0.9995,  0.9996, -1.0000,\n",
      "         0.9999, -0.9995,  0.6568,  1.0000,  0.9999,  0.9999, -0.9999,  0.9999,\n",
      "        -0.9998,  1.0000, -1.0000, -0.9998, -1.0000,  1.0000, -0.9999, -0.9981,\n",
      "        -1.0000,  0.9999, -0.9999,  1.0000,  0.9999, -0.9291,  1.0000, -0.9668,\n",
      "         0.9998, -0.9998,  0.9999, -1.0000,  0.9998,  0.9999, -0.9999, -0.9999,\n",
      "         0.9995,  1.0000,  1.0000,  0.9999, -1.0000,  0.9998, -1.0000,  0.9999,\n",
      "        -0.9999,  0.9999,  1.0000, -0.9999, -0.9999,  1.0000, -1.0000, -0.9999,\n",
      "        -1.0000, -1.0000,  0.9999, -1.0000], grad_fn=<TanhBackward>)\n",
      "Epoch 2000, loss -0.4255322217941284, out tensor([-1.0000, -0.9843,  1.0000, -1.0000, -1.0000, -0.9999, -1.0000, -1.0000,\n",
      "         1.0000, -0.9999, -1.0000, -0.9999,  1.0000,  0.9999, -1.0000, -0.9999,\n",
      "         1.0000, -1.0000,  0.9999,  0.9999,  0.8309, -0.9996, -1.0000, -0.9998,\n",
      "        -0.9998, -0.9998,  1.0000, -0.9998,  1.0000,  0.9998, -0.9999, -0.9964,\n",
      "        -0.9999,  1.0000,  1.0000,  1.0000,  0.9999,  1.0000,  0.9998,  0.9999,\n",
      "         0.9999, -0.9511,  1.0000,  0.9996, -0.9999,  0.9996,  0.9998, -0.9999,\n",
      "         1.0000, -0.9979, -0.8997,  1.0000,  0.9999,  0.9999, -0.9999,  0.9999,\n",
      "        -0.9999,  1.0000, -1.0000, -0.9999, -1.0000,  0.9999, -0.9999, -0.9849,\n",
      "        -0.9999,  1.0000, -1.0000,  1.0000,  1.0000, -0.8736,  0.9999, -0.9382,\n",
      "         0.9999, -0.9999,  0.9999, -1.0000,  0.9998,  0.9999, -1.0000, -0.9999,\n",
      "         0.9996,  1.0000,  1.0000,  0.9999, -1.0000,  0.9999, -1.0000,  0.9999,\n",
      "        -0.9999,  0.9999,  1.0000, -0.9999, -0.9999,  1.0000, -1.0000, -0.9999,\n",
      "        -1.0000, -1.0000,  1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "Epoch 3000, loss -0.4279806315898895, out tensor([-1.0000, -0.9955,  1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         1.0000, -0.9999, -1.0000, -0.9999,  1.0000,  0.9999, -1.0000, -0.9999,\n",
      "         1.0000, -1.0000,  0.9999,  1.0000,  0.5132, -0.9995, -1.0000, -0.8447,\n",
      "        -0.9999, -0.9997,  1.0000, -0.9998,  1.0000,  0.9999, -0.9999, -0.9273,\n",
      "        -0.9999,  0.9999,  1.0000,  1.0000,  0.9999,  1.0000,  0.9996,  0.9999,\n",
      "         1.0000, -0.9991,  1.0000,  0.9995, -0.9999,  0.9998,  0.9998, -0.9996,\n",
      "         1.0000, -0.9887, -0.2241,  1.0000,  0.9999,  0.9999, -1.0000,  0.9999,\n",
      "        -0.9999,  1.0000, -1.0000, -0.9999, -1.0000,  0.9999, -0.9999, -0.9988,\n",
      "        -0.9999,  1.0000, -1.0000,  1.0000,  1.0000, -0.5893,  0.9999, -0.9882,\n",
      "         0.9999, -0.9999,  0.9999, -1.0000,  0.9999,  0.9999, -1.0000, -0.9999,\n",
      "         0.9997,  1.0000,  1.0000,  0.9998, -1.0000,  0.9998, -1.0000,  1.0000,\n",
      "        -0.9999,  1.0000,  1.0000, -0.9999, -1.0000,  1.0000, -0.9999, -0.9999,\n",
      "        -1.0000, -1.0000,  1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "Epoch 4000, loss -0.4267420470714569, out tensor([-1.0000, -0.9995,  1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         1.0000, -0.9999, -1.0000, -0.9999,  1.0000,  0.9999, -1.0000, -0.9999,\n",
      "         1.0000, -1.0000,  0.9999,  1.0000, -0.8899, -0.9998, -1.0000,  0.7494,\n",
      "        -0.9999, -0.9996,  1.0000, -0.9998,  1.0000,  0.9999, -0.9999, -0.9536,\n",
      "        -0.9999,  1.0000,  1.0000,  1.0000,  0.9999,  1.0000,  0.9985,  1.0000,\n",
      "         1.0000, -0.9333,  1.0000,  0.9998, -0.9999,  0.9999,  0.9999, -0.9994,\n",
      "         1.0000, -0.9989, -0.9234,  1.0000,  0.9999,  0.9999, -1.0000,  0.9999,\n",
      "        -1.0000,  1.0000, -1.0000, -0.9999, -1.0000,  0.9999, -0.9999, -0.9998,\n",
      "        -0.9999,  1.0000, -1.0000,  1.0000,  1.0000, -0.9917,  0.9999, -0.9333,\n",
      "         0.9998, -0.9999,  1.0000, -1.0000,  0.9999,  0.9999, -1.0000, -1.0000,\n",
      "         0.9998,  1.0000,  1.0000,  0.9999, -1.0000,  0.9998, -1.0000,  1.0000,\n",
      "        -0.9999,  1.0000,  1.0000, -0.9999, -1.0000,  1.0000, -1.0000, -0.9999,\n",
      "        -1.0000, -1.0000,  1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "Epoch 5000, loss -0.42857450246810913, out tensor([-1.0000, -0.9610,  1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         1.0000, -0.9999, -1.0000, -0.9999,  1.0000,  0.9999, -1.0000, -0.9998,\n",
      "         1.0000, -1.0000,  0.9999,  1.0000, -0.8768, -0.9998, -1.0000,  0.4814,\n",
      "        -0.9999, -0.9995,  1.0000, -0.9998,  1.0000,  0.9999, -0.9999, -0.8439,\n",
      "        -0.9999,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.9216,  1.0000,\n",
      "         1.0000, -0.6207,  1.0000,  0.9998, -1.0000,  0.9999,  0.9999, -0.9990,\n",
      "         1.0000, -0.9993, -0.9604,  1.0000,  0.9999,  0.9999, -1.0000,  1.0000,\n",
      "        -1.0000,  1.0000, -1.0000, -0.9999, -1.0000,  0.9999, -0.9999, -0.9999,\n",
      "        -0.9999,  1.0000, -1.0000,  1.0000,  1.0000, -0.9830,  0.9999, -0.9172,\n",
      "         0.9997, -0.9999,  1.0000, -1.0000,  0.9999,  0.9999, -1.0000, -1.0000,\n",
      "         0.9998,  1.0000,  1.0000,  0.9999, -1.0000,  0.9999, -1.0000,  1.0000,\n",
      "        -0.9999,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000, -0.9999,\n",
      "        -1.0000, -1.0000,  1.0000, -1.0000], grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 进行生成网络的训练：\n",
    "out = genTrain(5000, genOpt, genNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0000, -0.9610,  1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         1.0000, -0.9999, -1.0000, -0.9999,  1.0000,  0.9999, -1.0000, -0.9998,\n",
       "         1.0000, -1.0000,  0.9999,  1.0000, -0.8768, -0.9998, -1.0000,  0.4814,\n",
       "        -0.9999, -0.9995,  1.0000, -0.9998,  1.0000,  0.9999, -0.9999, -0.8439,\n",
       "        -0.9999,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.9216,  1.0000,\n",
       "         1.0000, -0.6207,  1.0000,  0.9998, -1.0000,  0.9999,  0.9999, -0.9990,\n",
       "         1.0000, -0.9993, -0.9604,  1.0000,  0.9999,  0.9999, -1.0000,  1.0000,\n",
       "        -1.0000,  1.0000, -1.0000, -0.9999, -1.0000,  0.9999, -0.9999, -0.9999,\n",
       "        -0.9999,  1.0000, -1.0000,  1.0000,  1.0000, -0.9830,  0.9999, -0.9172,\n",
       "         0.9997, -0.9999,  1.0000, -1.0000,  0.9999,  0.9999, -1.0000, -1.0000,\n",
       "         0.9998,  1.0000,  1.0000,  0.9999, -1.0000,  0.9999, -1.0000,  1.0000,\n",
       "        -0.9999,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000, -0.9999,\n",
       "        -1.0000, -1.0000,  1.0000, -1.0000], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
